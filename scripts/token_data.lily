# This script generates the following:
#
# src/lily_lexer_tables.h
#
# * ch_table: This is used by lexer's lily_next_token to reduce the number of
#             cases to switch over. It's generated to allow optimizations.
#
# * ident_table: This holds characters that are valid identifiers. It's
#                generated to make it easy to verify.
#
# * token_name_table: This is used to print a name for a token. It's generated
#                     to make sure each token has the right representation.
#
# src/lily_token.h
#
# * lily_token: This is the token enum that lexer and others will use.
#

var script_name = "scripts/token_data.lily"

# Token records are in three parts:
#
# 1) A single character representation of the token, if one exists.
#    If this token does not map to a single Byte, this is ' '.
#
# 2) What to write when printing the token.
#
# 3) The name to write in the lily_token enum.
#
var token_data = [
    <[')',  ")",                "tk_right_parenth"]>,
    <[',',  ",",                "tk_comma"]>,
    <['{',  "{",                "tk_left_curly"]>,
    <['}',  "}",                "tk_right_curly"]>,
    <['[',  "[",                "tk_left_bracket"]>,
    <[':',  ":",                "tk_colon"]>,
    <['~',  "~",                "tk_tilde"]>,

    # These five have their eq at +1 from their base.
    <['^',  "^",                "tk_bitwise_xor"]>,
    <[' ',  "^=",               "tk_bitwise_xor_eq"]>,
    <['!',  "!",                "tk_not"]>,
    <[' ',  "!=",               "tk_not_eq"]>,
    <['%',  "%",                "tk_modulo"]>,
    <[' ',  "%=",               "tk_modulo_eq"]>,
    <['*',  "*",                "tk_multiply"]>,
    <[' ',  "*=",               "tk_multiply_eq"]>,
    <['/',  "/",                "tk_divide"]>,
    <[' ',  "/=",               "tk_divide_eq"]>,

    <['+',  "+",                "tk_plus"]>,
    <[' ',  "++",               "tk_plus_plus"]>,
    <[' ',  "+=",               "tk_plus_eq"]>,
    <['-',  "-",                "tk_minus"]>,
    <[' ',  "-=",               "tk_minus_eq"]>,
    <['<',  "<",                "tk_lt"]>,
    <[' ',  "<=",               "tk_lt_eq"]>,
    <[' ',  "<<",               "tk_left_shift"]>,
    <[' ',  "<<=",              "tk_left_shift_eq"]>,
    <['>',  ">",                "tk_gt"]>,
    <[' ',  ">=",               "tk_gt_eq"]>,
    <[' ',  ">>",               "tk_right_shift"]>,
    <[' ',  ">>=",              "tk_right_shift_eq"]>,
    <['=',  "=",                "tk_equal"]>,
    <[' ',  "==",               "tk_eq_eq"]>,
    <['(',  "(",                "tk_left_parenth"]>,
    <[' ',  "a lambda",         "tk_lambda"]>,
    <[' ',  "<[",               "tk_tuple_open"]>,
    <[' ',  "]>",               "tk_tuple_close"]>,
    <[']',  "]",                "tk_right_bracket"]>,
    <[' ',  "=>",               "tk_arrow"]>,
    <[' ',  "a label",          "tk_word"]>,
    <[' ',  "a property name",  "tk_prop_word"]>,
    <['"',  "a string",         "tk_double_quote"]>,
    <[' ',  "a bytestring",     "tk_bytestring"]>,
    <['\'', "a byte",           "tk_byte"]>,
    <[' ',  "an integer",       "tk_integer"]>,
    <[' ',  "a double",         "tk_double"]>,
    <[' ',  "a docblock",       "tk_docblock"]>,
    <[' ',  "a named argument", "tk_keyword_arg"]>,
    <['.',  ".",                "tk_dot"]>,
    <['&',  "&",                "tk_bitwise_and"]>,
    <[' ',  "&=",               "tk_bitwise_and_eq"]>,
    <[' ',  "&&",               "tk_logical_and"]>,
    <['|',  "|",                "tk_bitwise_or"]>,
    <[' ',  "|=",               "tk_bitwise_or_eq"]>,
    <[' ',  "||",               "tk_logical_or"]>,
    <[' ',  "@(",               "tk_typecast_parenth"]>,
    <[' ',  "...",              "tk_three_dots"]>,
    <[' ',  "|>",               "tk_func_pipe"]>,
    <[' ',  "$1",               "tk_scoop"]>,
    <[' ',  "invalid token",    "tk_invalid"]>,
    <[' ',  "end of lambda",    "tk_end_lambda"]>,
    <[' ',  "?>",               "tk_end_tag"]>,
    <[' ',  "end of file",      "tk_eof"]>,
]
var cc_names = [
    "CC_AT",
    "CC_B",
    "CC_CASH",
    "CC_DIGIT",
    "CC_NEWLINE",
    "CC_QUESTION",
    "CC_SHARP",
]
var token_data_size = 1 + token_data.size()
var cc_data: Hash[String, String] = []
for i in 0...cc_names.size() - 1: {
    var name = cc_names[i]

    cc_data[name] = (i + token_data_size).to_s()
}

define write_header(f: File, name: String)
{
    f.write("""\
#ifndef {0}
# define {0}

/* Generated by {1}. */\n\n\
    """.format(name, script_name))
}

define write_ch_defines(f: File)
{
    for i in 0...cc_names.size() - 1: {
        var cc = cc_names[i]
        var digit = cc_data[cc]
        var line = "# define " ++ cc ++ " " ++ digit ++ "\n"

        f.write(line)
    }

    f.write("\n")
}

define write_table_header(f: File, info: String, size: Integer)
{
    f.write("static const " ++ info ++ "[" ++ size ++ "] = {\n")
}

define write_numeric_table(f: File, table: List[String])
{
    for i in 0...255 by 16: {
        var slice = table.slice(i, i + 16)
                         .join(", ")

        slice = "    " ++ slice ++ ",\n"
        f.write(slice)
    }

    f.write("};\n\n")
}

define write_string_table(f: File, table: List[String])
{
    for i in 0...table.size() - 1: {
        table[i] |> f.write
    }

    f.write("};\n\n")
}

define token_id_by_name(name: String): String
{
    var i = 0

    for i in 0...token_data.size() - 1: {
        if token_data[i][2] == name: {
            break
        }
    }

    return i.to_s()
}

define set_range[A](ch_table: List[A], from: Integer, to: Integer, v: A)
{
    for i in from...to: {
        ch_table[i] = v
    }
}

define write_ch_table(f: File)
{
    var invalid_id = token_id_by_name("tk_invalid")
    var word_id = token_id_by_name("tk_word")
    var ch_table = List.repeat(256, invalid_id)

    # Load in ids for single character tokens and find the magic ones.
    for i in 0...token_data.size() - 1: {
        var ch = token_data[i][0]

        if ch == ' ': {
            continue
        }

        var digit = i.to_s()

        if i < 10: {
            digit = " " ++ digit
        }

        ch_table[ch] = digit
    }

    set_range(ch_table, 'a'.to_i(), 'z'.to_i(), word_id)
    set_range(ch_table, 'A'.to_i(), 'Z'.to_i(), word_id)
    set_range(ch_table, 0xC2,       0xF4,       word_id)
    set_range(ch_table, '0'.to_i(), '9'.to_i(), cc_data["CC_DIGIT"])

    ch_table['@']  = cc_data["CC_AT"]
    ch_table['B']  = cc_data["CC_B"]
    ch_table['$']  = cc_data["CC_CASH"]
    ch_table['\n'] = cc_data["CC_NEWLINE"]
    ch_table['?']  = cc_data["CC_QUESTION"]
    ch_table['#']  = cc_data["CC_SHARP"]
    ch_table['_']  = word_id

    write_table_header(f, "uint8_t ch_table", 256)
    write_numeric_table(f, ch_table)
}

define write_ident_defines(f: File)
{
    f.write("""\
# define IS_IDENT_START(x) (ident_table[x] == 1)\n\n\
    """)
}

define write_ident_table(f: File)
{
    # This table is generated to make it easier to verify.
    var ident_table = List.repeat(256, "0")

    set_range(ident_table, 'a'.to_i(), 'z'.to_i(), "1")
    set_range(ident_table, 'A'.to_i(), 'Z'.to_i(), "1")
    ident_table['_'] = "1"

    # Lexer's line reading verifies that utf-8 is properly formed.
    set_range(ident_table, 0xC2,       0xF4,       "1")
    set_range(ident_table, 0x80,       0xBF,       "1")

    # '@' uses this to check properties since 'B' is not tk_word.
    set_range(ident_table, '0'.to_i(), '9'.to_i(), "2")

    write_table_header(f, "uint8_t ident_table", 256)
    write_numeric_table(f, ident_table)
}

define write_token_name_table(f: File)
{
    var token_strings = token_data.map(|m| m[1] )
    var token_table: List[String] = []
    var current_line: List[String] = []

    # 4 spaces for indentation -1 because the first entry adds +2 for a space it
    # doesn't have.
    var line_length = 3

    for i in 0...token_strings.size() - 1: {
        var value = "\"" ++ token_strings[i] ++ "\""

        # +2 for comma and space.
        var value_size = value.size() + 2

        if line_length + value_size > 80: {
            var to_write = "    " ++ current_line.join(", ") ++ ",\n"

            token_table.push(to_write)
            current_line = []
            line_length = 3
        }

        current_line.push(value)
        line_length += value_size
    }

    # The token table as-is always has an unfinished line.
    var to_write = "    " ++ current_line.join(", ") ++ ",\n"

    token_table.push(to_write)

    write_table_header(f, "char *token_name_table", token_strings.size())
    write_string_table(f, token_table)
}

define write_token_enum(f: File)
{
    var token_names = token_data.map(|m| m[2] )

    f.write("typedef enum {\n")

    for i in 0...token_names.size() - 1: {
        var t = "    " ++ token_names[i] ++ ",\n"

        f.write(t)
    }

    f.write("} lily_token;\n\n")
}

define finish_file(f: File, path: String)
{
    f.write("#endif\n")
    f.close()
    print("scripts/token_data.lily: Updated " ++ path ++ ".")
}

define write_lily_lexer_data
{
    var path = "src/lily_lexer_data.h"
    var f = File.open(path, "w")

    write_header(f, "LILY_LEXER_DATA_H")
    write_ch_defines(f)
    write_ch_table(f)
    write_ident_defines(f)
    write_ident_table(f)
    write_token_name_table(f)
    finish_file(f, path)
}

define write_lily_token
{
    var path = "src/lily_token.h"
    var f = File.open(path, "w")

    write_header(f, "LILY_TOKEN_H")
    write_token_enum(f)
    finish_file(f, path)
}

write_lily_lexer_data()
write_lily_token()
