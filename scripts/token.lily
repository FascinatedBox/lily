# This script generates the following:
#
# src/lily_lexer_tables.h
#
# * ch_table: This is used by lexer's lily_next_token to reduce the number of
#             cases to switch over. It's generated to allow optimizations.
#
# * ident_table: This holds characters that are valid identifiers. It's
#                generated to make it easy to verify.
#
# * token_name_table: This is used to print a name for a token. It's generated
#                     to make sure each token has the right representation.
#
# src/lily_token.h
#
# * lily_token: This is the token enum that lexer and others will use.
#

import (finish_file,
        open_file,
        script_name,
        token_data) common

script_name = "scripts/token.lily"

var cc_names = [
    "CC_AT",
    "CC_B",
    "CC_CASH",
    "CC_DIGIT",
    "CC_NEWLINE",
    "CC_QUESTION",
    "CC_SHARP",
]
var token_data_size = 1 + token_data.size()
var cc_data: Hash[String, String] = []
for i in 0...cc_names.size() - 1: {
    var name = cc_names[i]

    cc_data[name] = (i + token_data_size).to_s()
}
var priority_data = [
    1 => [
        "tk_bitwise_and_eq",
        "tk_bitwise_or_eq",
        "tk_bitwise_xor_eq",
        "tk_divide_eq",
        "tk_eq_eq",
        "tk_equal",
        "tk_gt_eq",
        "tk_left_shift_eq",
        "tk_lt_eq",
        "tk_minus_eq",
        "tk_modulo_eq",
        "tk_multiply_eq",
        "tk_not_eq",
        "tk_plus_eq",
        "tk_right_shift_eq",
    ],
    2 => [
        "tk_logical_or",
    ],
    3 => [
        "tk_logical_and",
    ],
    4 => [
        "tk_eq_eq",
        "tk_not_eq",
        "tk_lt",
        "tk_gt",
        "tk_lt_eq",
        "tk_gt_eq",
    ],
    # Put concat here so it can chain together the output of pipe ops:
    # `a |> b ++ b |> c`.
    5 => [
        "tk_plus_plus",
    ],
    # Put pipes here so they capture as much as possible for comparison ops:
    # `a << b |> fn <= something`.
    6 => [
        "tk_func_pipe",
    ],
    7 => [
        "tk_bitwise_or",
        "tk_bitwise_xor",
        "tk_bitwise_and",
    ],
    8 => [
        "tk_left_shift",
        "tk_right_shift",
    ],
    9 => [
        "tk_plus",
        "tk_minus",
    ],
    10 => [
        "tk_multiply",
        "tk_divide",
        "tk_modulo",
    ],
]

define write_ch_defines(f: File)
{
    for i in 0...cc_names.size() - 1: {
        var cc = cc_names[i]
        var digit = cc_data[cc]
        var line = "# define " ++ cc ++ " " ++ digit ++ "\n"

        f.write(line)
    }

    f.write("\n")
}

define write_table_header(f: File, info: String, size: Integer)
{
    f.write("static const " ++ info ++ "[" ++ size ++ "] = {\n")
}

define write_numeric_table[A](f: File, table: List[A])
{
    for i in 0...table.size() - 1 by 16: {
        var slice = table.slice(i, i + 16)

        if slice.size() == 0: {
            slice = table.slice(i, table.size() - 1)
        }

        var slice_str = "    " ++ slice.join(", ") ++ ",\n"

        f.write(slice_str)
    }

    f.write("};\n\n")
}

define write_string_table(f: File, table: List[String])
{
    for i in 0...table.size() - 1: {
        table[i] |> f.write
    }

    f.write("};\n\n")
}

define token_id_by_name(name: String): String
{
    var i = 0

    for i in 0...token_data.size() - 1: {
        if token_data[i][2] == name: {
            break
        }
    }

    return i.to_s()
}

define set_range[A](ch_table: List[A], from: Integer, to: Integer, v: A)
{
    for i in from...to: {
        ch_table[i] = v
    }
}

define write_ch_table(f: File)
{
    var invalid_id = token_id_by_name("tk_invalid")
    var word_id = token_id_by_name("tk_word")
    var ch_table = List.repeat(256, invalid_id)

    # Load in ids for single character tokens and find the magic ones.
    for i in 0...token_data.size() - 1: {
        var ch = token_data[i][0]

        if ch == ' ': {
            continue
        }

        var digit = i.to_s()

        if i < 10: {
            digit = " " ++ digit
        }

        ch_table[ch] = digit
    }

    set_range(ch_table, 'a'.to_i(), 'z'.to_i(), word_id)
    set_range(ch_table, 'A'.to_i(), 'Z'.to_i(), word_id)
    set_range(ch_table, 0xC2,       0xF4,       word_id)
    set_range(ch_table, '0'.to_i(), '9'.to_i(), cc_data["CC_DIGIT"])

    ch_table['@']  = cc_data["CC_AT"]
    ch_table['B']  = cc_data["CC_B"]
    ch_table['$']  = cc_data["CC_CASH"]
    ch_table['\n'] = cc_data["CC_NEWLINE"]
    ch_table['?']  = cc_data["CC_QUESTION"]
    ch_table['#']  = cc_data["CC_SHARP"]
    ch_table['_']  = word_id

    write_table_header(f, "uint8_t ch_table", 256)
    write_numeric_table(f, ch_table)
}

define write_ident_defines(f: File)
{
    f.write("""\
# define IS_IDENT_START(x) (ident_table[x] == 1)\n\n\
    """)
}

define write_ident_table(f: File)
{
    # This table is generated to make it easier to verify.
    var ident_table = List.repeat(256, "0")

    set_range(ident_table, 'a'.to_i(), 'z'.to_i(), "1")
    set_range(ident_table, 'A'.to_i(), 'Z'.to_i(), "1")
    ident_table['_'] = "1"

    # Lexer's line reading verifies that utf-8 is properly formed.
    set_range(ident_table, 0xC2,       0xF4,       "1")
    set_range(ident_table, 0x80,       0xBF,       "1")

    # '@' uses this to check properties since 'B' is not tk_word.
    set_range(ident_table, '0'.to_i(), '9'.to_i(), "2")

    write_table_header(f, "uint8_t ident_table", 256)
    write_numeric_table(f, ident_table)
}

define write_token_name_table(f: File)
{
    var token_strings = token_data.map(|m| m[1] )
    var token_table: List[String] = []
    var current_line: List[String] = []

    # 4 spaces for indentation -1 because the first entry adds +2 for a space it
    # doesn't have.
    var line_length = 3

    for i in 0...token_strings.size() - 1: {
        var value = "\"" ++ token_strings[i] ++ "\""

        # +2 for comma and space.
        var value_size = value.size() + 2

        if line_length + value_size > 80: {
            var to_write = "    " ++ current_line.join(", ") ++ ",\n"

            token_table.push(to_write)
            current_line = []
            line_length = 3
        }

        current_line.push(value)
        line_length += value_size
    }

    # The token table as-is always has an unfinished line.
    var to_write = "    " ++ current_line.join(", ") ++ ",\n"

    token_table.push(to_write)

    write_table_header(f, "char *token_name_table", token_strings.size())
    write_string_table(f, token_table)
}

define write_priority_table(f: File)
{
    var priority_table = List.repeat(token_data.size(), 0)
    var priority_keys = priority_data.keys()
    var id_map: Hash[String, Integer] = []

    for i in 0...token_data.size() - 1: {
        id_map[token_data[i][2]] = i
    }

    # The priority table is a Hash keyed by priority.
    for i in 0...priority_keys.size() - 1: {
        var key = priority_keys[i]
        var token_list = priority_data[key]

        for j in 0...token_list.size() - 1: {
            var token = token_list[j]

            priority_table[id_map[token]] = key
        }
    }

    write_table_header(f, "uint8_t priority_table", priority_table.size())
    write_numeric_table(f, priority_table)
}

define write_token_enum(f: File)
{
    var token_names = token_data.map(|m| m[2] )

    f.write("typedef enum {\n")

    for i in 0...token_names.size() - 1: {
        var t = "    " ++ token_names[i] ++ ",\n"

        f.write(t)
    }

    f.write("} lily_token;\n")

    f.write("""\n\
# define IS_ASSIGN_TOKEN(t) (lily_priority_for_token(t) == 1)
# define IS_COMPARE_TOKEN(t) (lily_priority_for_token(t) == 4)

int lily_priority_for_token(lily_token);
const char *tokname(lily_token);\n\n\
    """)
}

define write_lily_lexer_data
{
    var path = "src/lily_lexer_data.h"
    var f = open_file(path)

    write_ch_defines(f)
    write_ch_table(f)
    write_ident_defines(f)
    write_ident_table(f)
    write_token_name_table(f)
    write_priority_table(f)
    finish_file(f, path)
}

define write_lily_token
{
    var path = "src/lily_token.h"
    var f = open_file(path)

    write_token_enum(f)
    finish_file(f, path)
}

write_lily_lexer_data()
write_lily_token()
